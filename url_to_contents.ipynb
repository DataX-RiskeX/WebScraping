{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://altair-viz.github.io/case_studies/exploring-weather.html\n",
    "# look into this once you get started with EDA\n",
    "# conda install -c conda-forge altair \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Packages and Libraries ##\n",
    "\n",
    "# Web parcing, scraping, etc.\n",
    "import bs4 as bs # BeautifulSoup4 \n",
    "import urllib3\n",
    "import re\n",
    "import requests # HTTP parser\n",
    "import html5lib\n",
    "\n",
    "# DataFrames and math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Output related packages \n",
    "import pprint as pp\n",
    "\n",
    "# read-in and write-out\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stretch Jupyter coding blocks to fit screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\")) \n",
    "\n",
    "# make it run on py2 and py3\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining II\n",
    "This  notebook is intended to perform the following processes:\n",
    "\n",
    "    2.1 Read-in batched csv files and perform content extraction.\n",
    "\n",
    "    2.2 The notebook uses beautifulsoup to extract paragraph content of each url in batch.\n",
    "\n",
    "    2.3 Content extraction is written out as a matching csv file for future concatenation/merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **Begin Data Mining II:** Per-url, article content extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.1 Read-in batched csv files and perform content extraction.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all .csv elements in folder\n",
    "import glob\n",
    "csv_all = glob.glob('*.csv') # elements of list are string name for each .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.2 Use beautifulsoup to extract paragraph content of each url in batch.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **get_article**\n",
    "get_artilcle controls two function that extract the contents of a given url -- by accessing the paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_site(site):\n",
    "    from urllib.parse import urlparse\n",
    "    import copy\n",
    "    url = copy.copy(site) \n",
    "    \n",
    "    # urlparse requires a string \n",
    "    info = urlparse(str(url))\n",
    "    if 'mp3' in info.path:            # validates that site is not a podcast\n",
    "        return('')                    # flagged for removal\n",
    "    else:\n",
    "        return(url)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(site):\n",
    "    src = requests.get(site).content            # accesses content of html object\n",
    "    soup = bs.BeautifulSoup(src, 'lxml')        # object creation used in extracting paragraphs using built-in html parser\n",
    "    body = soup.find_all('p')                   # finds all paragraphs '<p>' in html object\n",
    "    return(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_content(body):\n",
    "    sentence = [parags.text for parags in body]\n",
    "    text = \"\\t\".join(sentence)                  # tab delimeter for easier extraction\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article(site):\n",
    "    valid_site = validate_site(site)\n",
    "    if not valid_site:\n",
    "        return('.mp3: Invalid Type')                # uses 'falsy' check if string is empty \n",
    "    else:\n",
    "        body = get_content(valid_site)\n",
    "        text = extract_from_content(body)\n",
    "        if not text:                                # uses 'falsy' check if string is empty \n",
    "            return('403 Forbidden')\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **get_text**\n",
    "Function loops over all urls and creates a list to be appended to newsDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(df):\n",
    "    \"\"\"\n",
    "    Accepts a 6xn dataframe and returns a 1xn list and a modifed dataframe\n",
    "    Note: stitching a dataframes to a list is much faster that updating existing one with a new column\n",
    "    \"\"\"\n",
    "    import copy\n",
    "\n",
    "    content = []\n",
    "    url = []\n",
    "    author = copy.copy(df['author'])\n",
    "    link = copy.copy(df['source_url'])\n",
    "    \n",
    "    if author == 'Ml-implode.com': \n",
    "        r = requests.get(link)\n",
    "        soup = bs.BeautifulSoup(r.text, 'html.parser')\n",
    "        source = soup.find(id=\"LIJIT_title\")            # manually found -- may differ moving forward\n",
    "        link = source.find('a').get('href')             # gets the 'href inside the a tag -- i.e. the correct url\n",
    "        url.append(link)\n",
    "        content.append(get_article(link))               # send correct url out for extraction\n",
    "        \n",
    "    else:\n",
    "        content.append(get_article(link))\n",
    "        \n",
    "    return({'url':url, 'content':content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **combine_df**\n",
    "Function sends list of urls to get_text and get_articles for content extraction. Function then combines all data into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_df(df1, df2):\n",
    "    import copy\n",
    "    author = copy.deepcopy(df1['author'])\n",
    "    df = copy.deepcopy(df1)\n",
    "    df['contents'] = \"\".join(df2['content']) # converts list to string\n",
    "    \n",
    "    # catches aggregator names and replaces them with that of the buplisher\n",
    "    #checks for know aggregator, replaces it with the url referenced by aggregator\n",
    "    for i in range(len(df)):\n",
    "        if author == 'Ml-implode.com':\n",
    "            df['source_url'] = \"\".join(df2['url']) # converts 'list' item to 'str' item\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_author_name(df):\n",
    "    from urllib.parse import urlparse\n",
    "    import copy\n",
    "    #data = copy(df)\n",
    "    url = copy.copy(df) \n",
    "    \n",
    "    # urlparse requires a string \n",
    "    info = urlparse(str(url))\n",
    "    return(info.netloc)          #.netloc extract the main url -- i.e. excludes path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'None' values\n",
    "def rm_false_author(df):\n",
    "    \"\"\"\n",
    "     Receives pandas datraframe, and removes aggregator name from author feature\n",
    "    \"\"\"\n",
    "    author = df['author']\n",
    "    source = df['source_url']\n",
    "    publisher = df['publisher']\n",
    "    \n",
    "    #checks for know aggregator, replaces it with the url referenced by aggregator\n",
    "    for i in range(len(df)):\n",
    "        if author.loc[i] == 'Ml-implode.com':\n",
    "            author.loc[i] = replace_author_name(source.loc[i])  # parameter is correct url as type 'str'\n",
    "            publisher.loc[i] = author.loc[i]                    #replaces instances of incorrect publisher name\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_df(df):\n",
    "    import copy\n",
    "    df = copy.deepcopy(df)\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # for-loop merges dataframes and calls other functions that provide dataframe with data\n",
    "    for i in range(len(df)):\n",
    "        df1 = copy.deepcopy(df.loc[i])\n",
    "        df2 = get_text(df1)\n",
    "        datum = data.append(combine_df(df1,df2))\n",
    "        \n",
    "        ## minor clean up ##\n",
    "        # optional: data = data.drop(columns=['description'])     # removes redundant column        \n",
    "        data = rm_false_author(datum)                             # replaces invalid author entry\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__do something__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes rows with inaccessible data\n",
    "def rm_site_error(df):\n",
    "    \"\"\"\n",
    "     Receives pandas datraframe, and removes rows containing data not accessible by BeautifulSoup\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    data = copy.deepcopy(df)\n",
    "    content = data['contents']\n",
    "    for i in range(len(content)):\n",
    "        if content.loc[i] == '403 Forbidden':\n",
    "            print(\"Error Found: 403 Forbidden --> Dropping site from dataframe...\")\n",
    "            # df.drop(df.index[[1,3]])\n",
    "            data.drop([i], axis = 0, inplace = True)\n",
    "            #df = data[data.index != '403 Forbidden']\n",
    "            #print(data.index[i], type(data.index[i]))\n",
    "            #iris.ix[iris['sepal length (cm)'] >= 5]\n",
    "            #data = df.ix[df['contents'] == '403 Forbidden' ]\n",
    "        elif content.loc[i] == '.mp3: Invalid Type':\n",
    "            print(\"Error Found: .mp3 Invalid Type --> Dropping site from dataframe...\")\n",
    "            data.drop([i], axis = 0, inplace = True)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed Process: \n",
    "##### riskEx_df represents a dataframe of extracted features, their values, and the text content of each article's body\n",
    "\n",
    "__The following cell has an approximate run time of:__ \n",
    "#### **5 HOURS**\n",
    "\n",
    "Use *riskEx_df.csv* or *riskEx_df.json* instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control(lst):\n",
    "    for item in range(len(lst)):\n",
    "        #print(item, lst[item])\n",
    "        \n",
    "        #extract_this = stitch_df(lst[item])\n",
    "        #clean_this = rm_site_error(exract_this) \n",
    "        #item.to_csv('riskEx_{id}.csv'.format(id=id), index_label = False)\n",
    "    #for id, df_i in  enumerate(np.array_split(df, batchSize)):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rawData_0.csv\n",
      "1 rawData_27.csv\n",
      "2 rawData_22.csv\n",
      "3 rawData_15.csv\n",
      "4 rawData_5.csv\n",
      "5 rawData_1.csv\n",
      "6 rawData_11.csv\n",
      "7 rawData_3.csv\n",
      "8 rawData_7.csv\n",
      "9 rawData_25.csv\n",
      "10 rawData_6.csv\n",
      "11 rawData_18.csv\n",
      "12 rawData_26.csv\n",
      "13 rawData_10.csv\n",
      "14 rawData_14.csv\n",
      "15 rawData_12.csv\n",
      "16 rawData_17.csv\n",
      "17 rawData_16.csv\n",
      "18 rawData_4.csv\n",
      "19 rawData_8.csv\n",
      "20 rawData_19.csv\n",
      "21 rawData_21.csv\n",
      "22 rawData_23.csv\n",
      "23 rawData_9.csv\n",
      "24 rawData_28.csv\n",
      "25 rawData_2.csv\n",
      "26 rawData_20.csv\n",
      "27 rawData_13.csv\n",
      "28 rawData_24.csv\n"
     ]
    }
   ],
   "source": [
    "control(csv_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.3 Write out content as matching csv file for future concatenation/merging__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ *riskEx_df* object is a 7xn DataFrame containing the following: \n",
    "\n",
    "All data from *newsDF* object. The content of the object *text*, column-appended according to the indices of newsDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **End Data Mining II:** Per-url, article content extraction\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
